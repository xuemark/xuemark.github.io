[
{
	"uri": "https://xuemark.github.io/",
	"title": "EKS Workshop",
	"tags": [],
	"description": "",
	"content": "EKS Workshop I am a good man.\n This is my record for workshop\n  Inspector Workshop\n  EKS Workshop\n  1. Login to AWS Console 2. Install tools for EKS 3. Create EKS cluster and nodegroup 4. Docker Image Build 5. Create APP/Deployment/Service 6. Create ALB Ingress Controller 7. Create service account for pod 8. Scale pod with HPA and CA "
},
{
	"uri": "https://xuemark.github.io/1.login-to-aws-console/",
	"title": "1. Login to AWS Console",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n  在浏览器中，打开链接https://dashboard.eventengine.run/login?hash=xxxxxx 点击“AWS Console” 点击“Open AWS Console” 点击右上角“Services”，然后搜索“Systems Manager” 点击打开Systems Manager 在左面导航栏中，找到Session Manager 点击进入Session Manager 点击右侧“Start Session” 选中“TestInstance”，点击“Start Session” 进入了EC2 instance。 执行下列命令  export PS1=\u0026quot;\\n[\\u@\\h \\W]$ \u0026quot; cd /home/ssm-user sudo su "
},
{
	"uri": "https://xuemark.github.io/2.install-tools-for-eks/",
	"title": "2. Install Tools For EKS",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n yum install yum install -y jq yum install -y httpd-tools install eksctl curl -OL \u0026quot;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026quot; tar -zxf eksctl_$(uname -s)_amd64.tar.gz mv ./eksctl /usr/bin eksctl version install kubectl curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.16.8/2020-04-16/bin/linux/amd64/kubectl chmod +x ./kubectl mv ./kubectl /usr/bin kubectl version "
},
{
	"uri": "https://xuemark.github.io/3.create-eks-cluster-and-nodegroup/",
	"title": "3. Create EKS Cluster and Nodegroup",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n 1. Environment export AWS_REGION=ap-southeast-1 export CLUSTER_NAME=eks-test export NODEGROUP=$CLUSTER_NAME-nodegroup export TOMCAT_VERSION=9.0.37 2. get subnet-id  点击左上角Services，搜索VPC。 点击VPC进入。 点击左侧Subnets。 查看Public Subnet x和Private Subnet x的Subnet ID。 用该subnet id更新下面的指令。 或者用下面的命令  aws ec2 describe-subnets --filters Name=vpc-id,Values=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=VPC-Test --region $AWS_REGION| jq -r '.Vpcs[0].VpcId') --region $AWS_REGION --query \u0026quot;Subnets[*].{SubnetId:SubnetId,SubnetName:Tags[?Key=='Name'].Value}\u0026quot; 3. Create EKS Cluster and Nodegroup eksctl create cluster \\ --name $CLUSTER_NAME \\ --version 1.16 \\ --region $AWS_REGION \\ --vpc-public-subnets subnet-xxx,subnet-xxx,subnet-xxx \\ --vpc-private-subnets subnet-xxx,subnet-xxx,subnet-xxx \\ --nodegroup-name $CLUSTER_NAME-nodegroup \\ --node-type t3.small \\ --node-volume-size 5 \\ --nodes 1 \\ --nodes-min 1 \\ --nodes-max 5 \\ --node-private-networking \\ --alb-ingress-access \\ --managed \\ --asg-access \\ --full-ecr-access wait about 10-15 minutes\n4. check node group status kubectl get node output\nNAME STATUS ROLES AGE VERSION ip-192-168-14-19.cn-northwest-1.compute.internal Ready \u0026lt;none\u0026gt; 4d1h v1.14.9-eks-1f0ca9 "
},
{
	"uri": "https://xuemark.github.io/4.docker-image-build/",
	"title": "4. Docker Imager Build",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n Build Docker Image including JAVA, Tomcat and AWSCLI\n1. install docker yum install -y docker systemctl start docker 2. download and install tomcat curl -o tomcat.tar.gz https://ftp.tsukuba.wide.ad.jp/software/apache/tomcat/tomcat-9/v${TOMCAT_VERSION}/bin/apache-tomcat-${TOMCAT_VERSION}.tar.gz tar -xzf tomcat.tar.gz mv apache-tomcat-${TOMCAT_VERSION} tomcat 3. make dockerfile cat \u0026lt;\u0026lt;EOF \u0026gt; dockerfile from centos:8 WORKDIR /home COPY tomcat/ /home/tomcat/ RUN yum install -y java-1.8.0-openjdk \u0026amp;\u0026amp;\\ yum install -y python36 \u0026amp;\u0026amp;\\ pip3 install awscli EXPOSE 8080 ENTRYPOINT [\u0026quot;/home/tomcat/bin/catalina.sh\u0026quot;,\u0026quot;run\u0026quot;] EOF 4. docker build docker build -t mytomcat . docker images output\nEPOSITORY TAG IMAGE ID CREATED SIZE mytomcat latest 65916bfdf6b7 7 seconds ago 550MB centos 8 470671670cac 4 months ago 237MB 5. docker run docker run -d -p 8080:8080 --name mytomcat-container mytomcat curl -I http://localhost:8080 6. upload docker image to ECR Initiate Elastic Container Registry  点击左上角“Services”，搜索“Elastic Container Registry” 进入Elastic Container Registry 点击右侧“get started” 在Repository name处，输入mytomcat，然后点击“Create repository” 记录URI，如xxxxxx.dkr.ecr.ap-southeast-1.amazonaws.com/mytomcat  upload docker image export ECR_URL=$(aws sts get-caller-identity| jq -r '.Account').dkr.ecr.ap-southeast-1.amazonaws.com/mytomcat $(aws ecr get-login --no-include-email --region ap-southeast-1) docker tag mytomcat:latest $ECR_URL:1 docker push $ECR_URL:1 "
},
{
	"uri": "https://xuemark.github.io/5.create-app/",
	"title": "5. Create APP Deployment Service",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n 1. make app myapp.yml update image like xxxxxx.dkr.ecr.ap-southeast-1.amazonaws.com/mytomcat:1\necho $ECR_URL APP Yaml\ncat \u0026lt;\u0026lt;EOF \u0026gt; myapp.yml --- apiVersion: apps/v1 kind: Deployment metadata: name: myapp-deployment labels: app: myapp spec: selector: matchLabels: app: myapp replicas: 1 template: metadata: labels: app: myapp spec: containers: - name: mytomcat image: xxxxxx.dkr.ecr.ap-southeast-1.amazonaws.com/mytomcat:1 ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: \u0026quot;myapp-service\u0026quot; spec: selector: app: myapp type: ClusterIP ports: - protocol: TCP port: 8080 targetPort: 8080 EOF 2. build deploy/service kubectl apply -f myapp.yml 3. check deployment and service kubectl get deploy kubectl get pod -o wide kubectl get service -o wide "
},
{
	"uri": "https://xuemark.github.io/6.create-alb-ingress-controller/",
	"title": "6. Create ALB Ingress Controller",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n 1. Create EKS OIDC Provider eksctl utils associate-iam-oidc-provider --cluster=${CLUSTER_NAME} --approve --region ${AWS_REGION} 2. Create IAM Policy create iam-policy-v1.1.5.json cat \u0026lt;\u0026lt;EOF \u0026gt; iam-policy-v1.1.5.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;acm:DescribeCertificate\u0026quot;, \u0026quot;acm:ListCertificates\u0026quot;, \u0026quot;acm:GetCertificate\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:AuthorizeSecurityGroupIngress\u0026quot;, \u0026quot;ec2:CreateSecurityGroup\u0026quot;, \u0026quot;ec2:CreateTags\u0026quot;, \u0026quot;ec2:DeleteTags\u0026quot;, \u0026quot;ec2:DeleteSecurityGroup\u0026quot;, \u0026quot;ec2:DescribeAccountAttributes\u0026quot;, \u0026quot;ec2:DescribeAddresses\u0026quot;, \u0026quot;ec2:DescribeInstances\u0026quot;, \u0026quot;ec2:DescribeInstanceStatus\u0026quot;, \u0026quot;ec2:DescribeInternetGateways\u0026quot;, \u0026quot;ec2:DescribeNetworkInterfaces\u0026quot;, \u0026quot;ec2:DescribeSecurityGroups\u0026quot;, \u0026quot;ec2:DescribeSubnets\u0026quot;, \u0026quot;ec2:DescribeTags\u0026quot;, \u0026quot;ec2:DescribeVpcs\u0026quot;, \u0026quot;ec2:ModifyInstanceAttribute\u0026quot;, \u0026quot;ec2:ModifyNetworkInterfaceAttribute\u0026quot;, \u0026quot;ec2:RevokeSecurityGroupIngress\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;elasticloadbalancing:AddListenerCertificates\u0026quot;, \u0026quot;elasticloadbalancing:AddTags\u0026quot;, \u0026quot;elasticloadbalancing:CreateListener\u0026quot;, \u0026quot;elasticloadbalancing:CreateLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:CreateRule\u0026quot;, \u0026quot;elasticloadbalancing:CreateTargetGroup\u0026quot;, \u0026quot;elasticloadbalancing:DeleteListener\u0026quot;, \u0026quot;elasticloadbalancing:DeleteLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:DeleteRule\u0026quot;, \u0026quot;elasticloadbalancing:DeleteTargetGroup\u0026quot;, \u0026quot;elasticloadbalancing:DeregisterTargets\u0026quot;, \u0026quot;elasticloadbalancing:DescribeListenerCertificates\u0026quot;, \u0026quot;elasticloadbalancing:DescribeListeners\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancers\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancerAttributes\u0026quot;, \u0026quot;elasticloadbalancing:DescribeRules\u0026quot;, \u0026quot;elasticloadbalancing:DescribeSSLPolicies\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTags\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTargetGroups\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTargetGroupAttributes\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTargetHealth\u0026quot;, \u0026quot;elasticloadbalancing:ModifyListener\u0026quot;, \u0026quot;elasticloadbalancing:ModifyLoadBalancerAttributes\u0026quot;, \u0026quot;elasticloadbalancing:ModifyRule\u0026quot;, \u0026quot;elasticloadbalancing:ModifyTargetGroup\u0026quot;, \u0026quot;elasticloadbalancing:ModifyTargetGroupAttributes\u0026quot;, \u0026quot;elasticloadbalancing:RegisterTargets\u0026quot;, \u0026quot;elasticloadbalancing:RemoveListenerCertificates\u0026quot;, \u0026quot;elasticloadbalancing:RemoveTags\u0026quot;, \u0026quot;elasticloadbalancing:SetIpAddressType\u0026quot;, \u0026quot;elasticloadbalancing:SetSecurityGroups\u0026quot;, \u0026quot;elasticloadbalancing:SetSubnets\u0026quot;, \u0026quot;elasticloadbalancing:SetWebACL\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;iam:CreateServiceLinkedRole\u0026quot;, \u0026quot;iam:GetServerCertificate\u0026quot;, \u0026quot;iam:ListServerCertificates\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;cognito-idp:DescribeUserPoolClient\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;waf-regional:GetWebACLForResource\u0026quot;, \u0026quot;waf-regional:GetWebACL\u0026quot;, \u0026quot;waf-regional:AssociateWebACL\u0026quot;, \u0026quot;waf-regional:DisassociateWebACL\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;tag:GetResources\u0026quot;, \u0026quot;tag:TagResources\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;waf:GetWebACL\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } EOF create IAM policy aws iam create-policy --policy-name ALBIngressControllerIAMPolicy \\ --policy-document file://./iam-policy-v1.1.5.json --region ${AWS_REGION} export ENV POLICY_NAME=$(aws iam list-policies --query 'Policies[?PolicyName==`ALBIngressControllerIAMPolicy`].Arn' --output text --region ${AWS_REGION}) 3.create service account eksctl create iamserviceaccount \\ --cluster=${CLUSTER_NAME} \\ --namespace=kube-system \\ --name=alb-ingress-controller \\ --attach-policy-arn=${POLICY_NAME} \\ --override-existing-serviceaccounts \\ --approve 4.create RBAC create rbac-role-v1.1.5.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; rbac-role-v1.1.5.yaml --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/name: alb-ingress-controller name: alb-ingress-controller rules: - apiGroups: - \u0026quot;\u0026quot; - extensions resources: - configmaps - endpoints - events - ingresses - ingresses/status - services verbs: - create - get - list - update - watch - patch - apiGroups: - \u0026quot;\u0026quot; - extensions resources: - nodes - pods - secrets - services - namespaces verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/name: alb-ingress-controller name: alb-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: alb-ingress-controller subjects: - kind: ServiceAccount name: alb-ingress-controller namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: labels: app.kubernetes.io/name: alb-ingress-controller name: alb-ingress-controller namespace: kube-system ... EOF create RBAC kubectl apply -f rbac-role-v1.1.5.yaml 5. create ALB APP get vpc-id eksctl get cluster --name eks-test -o json | jq -r '.[].ResourcesVpcConfig.VpcId' create alb-ingress-controller-v1.1.5.yaml modify\n \u0026ndash;cluster-name=xxx \u0026ndash;aws-vpc-id=xxx \u0026ndash;aws-region=xxx  cat \u0026lt;\u0026lt;EOF \u0026gt; alb-ingress-controller-v1.1.5.yaml # Application Load Balancer (ALB) Ingress Controller Deployment Manifest. # This manifest details sensible defaults for deploying an ALB Ingress Controller. # GitHub: https://github.com/kubernetes-sigs/aws-alb-ingress-controller apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: alb-ingress-controller name: alb-ingress-controller # Namespace the ALB Ingress Controller should run in. Does not impact which # namespaces it's able to resolve ingress resource for. For limiting ingress # namespace scope, see --watch-namespace. namespace: kube-system spec: selector: matchLabels: app.kubernetes.io/name: alb-ingress-controller template: metadata: labels: app.kubernetes.io/name: alb-ingress-controller spec: containers: - name: alb-ingress-controller args: # Limit the namespace where this ALB Ingress Controller deployment will # resolve ingress resources. If left commented, all namespaces are used. # - --watch-namespace=your-k8s-namespace # Setting the ingress-class flag below ensures that only ingress resources with the # annotation kubernetes.io/ingress.class: \u0026quot;alb\u0026quot; are respected by the controller. You may # choose any class you'd like for this controller to respect. - --ingress-class=alb # REQUIRED # Name of your cluster. Used when naming resources created # by the ALB Ingress Controller, providing distinction between # clusters. - --cluster-name=eks-test # AWS VPC ID this ingress controller will use to create AWS resources. # If unspecified, it will be discovered from ec2metadata. - --aws-vpc-id=vpc-07d32d2994e87f2cd # AWS region this ingress controller will operate in. # If unspecified, it will be discovered from ec2metadata. # List of regions: http://docs.aws.amazon.com/general/latest/gr/rande.html#vpc_region - --aws-region=ap-southeast-1 # Enables logging on all outbound requests sent to the AWS API. # If logging is desired, set to true. # - --aws-api-debug # Maximum number of times to retry the aws calls. # defaults to 10. # - --aws-max-retries=10 # env: # AWS key id for authenticating with the AWS API. # This is only here for examples. It's recommended you instead use # a project like kube2iam for granting access. #- name: AWS_ACCESS_KEY_ID # value: KEYVALUE # AWS key secret for authenticating with the AWS API. # This is only here for examples. It's recommended you instead use # a project like kube2iam for granting access. #- name: AWS_SECRET_ACCESS_KEY # value: SECRETVALUE # Repository location of the ALB Ingress Controller. image: docker.io/amazon/aws-alb-ingress-controller:v1.1.5 serviceAccountName: alb-ingress-controller EOF create ALB deployment kubectl apply -f alb-ingress-controller-v1.1.5.yaml view pod kubectl get pod -A 6. create ingress for service - create ALB check subnet tags  点击左上角Services，搜索VPC。 点击VPC进入。 点击左侧Subnets。 点击Public Subnet x。 查看下面窗口的Tags，是否包含kubernetes.io/role/elb 如果没有，点击add/edit tags. 然后点击create tags，输入key = kubernetes.io/role/elb ， Value = 1  create alb-ingress.yml cat \u0026lt;\u0026lt;EOF \u0026gt; alb-ingress.yml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: \u0026quot;alb-ingress\u0026quot; namespace: \u0026quot;default\u0026quot; annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip labels: app: myapp spec: rules: - http: paths: - path: /* backend: serviceName: \u0026quot;myapp-service\u0026quot; servicePort: 8080 EOF create ALB kubectl apply -f alb-ingress.yml 7. get ALB URL kubectl get ingress output\nAME HOSTS ADDRESS PORTS AGE alb-ingress * xxx.ap-southeast-1.elb.amazonaws.com 80 13m browser\nhttp://xxx.ap-southeast-1.elb.amazonaws.com "
},
{
	"uri": "https://xuemark.github.io/7.create-service-account/",
	"title": "7. Create service account for pod",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n 1. create service account Check OIDC Provider\neksctl create iamserviceaccount --name s3-full-access --namespace default \\ --cluster ${CLUSTER_NAME} --attach-policy-arn arn:aws-cn:iam::aws:policy/AmazonS3FullAccess \\ --approve --override-existing-serviceaccounts --region ${AWS_REGION} 2. create S3 bucket modify S3_BUCKET with unique name\nS3_BUCKET=ekstest20200715 create bucket\nif [ $(aws s3 ls | grep $S3_BUCKET | wc -l) -eq 0 ]; then aws s3 mb s3://$S3_BUCKET --region $AWS_REGION echo \u0026quot;test\u0026quot; \u0026gt; test.txt aws s3 cp test.txt s3://$S3_BUCKET/ else echo \u0026quot;S3 bucket $S3_BUCKET existed, skip creation\u0026quot; fi 2. enter pod kubectl get pod #kubectl exec -it \u0026lt;podname\u0026gt; /bin/bash kubectl exec -it $(kubectl get pod -o json | jq -r '.items[0].metadata.name') /bin/bash aws s3 ls s3://\u0026lt;S3_BUCKET\u0026gt;/ output\naccess denied 3. update myapp-deployment kubectl edit deployment myapp-deployment add \u0026ldquo;serviceAccountName: s3-full-access\u0026rdquo; below \u0026ldquo;terminationGracePeriodSeconds: 30\u0026rdquo;\n4. re-enter pod kubectl get pod kubectl exec -it $(kubectl get pod -o json | jq -r '.items[0].metadata.name') /bin/bash aws s3 ls s3://\u0026lt;S3_BUCKET\u0026gt; output\ntest.txt "
},
{
	"uri": "https://xuemark.github.io/8.scale-pod-node/",
	"title": "8. Scale pod and node",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n 1. manually scale pod kubectl scale --replicas=10 deployment/myapp-deployment kubectl get pod -o wide Check EC2\u0026gt;LOAD BALANCING\u0026gt;Target Group\u0026gt;Targets\n2. check pod status There are 4 pod in Pending status.\nkubectl get pod kubectl get pod -o json | jq -r '.items[] | select(.status.phase==\u0026quot;Pending\u0026quot;)' | jq -r '.metadata.name' kubectl get events 3. scale node group scale node instance to 2\neksctl scale nodegroup --cluster=${CLUSTER_NAME} --nodes=2 --name=${NODEGROUP} --region=${AWS_REGION} kubectl get node 4. check pod status All pods are in Running status.\nkubectl get pod 5. create HPA curl -sL https://api.github.com/repos/kubernetes-sigs/metrics-server/tarball/v0.3.6 -o metrics-server-v0.3.6.tar.gz tar -xzf metrics-server-v0.3.6.tar.gz kubectl apply -f kubernetes-sigs-metrics-server-d1f4f6f/deploy/1.8+/ kubectl get pod --all-namespaces 6. scale down pod kubectl scale --replicas=1 deployment/myapp-deployment 7. set limit for pod kubectl set resources deployment myapp-deployment --limits=cpu=200m,memory=256Mi 8. set autoscale kubectl autoscale deployment myapp-deployment --cpu-percent=30 --min=1 --max=20 9. monitor HPA kubectl top pod kubectl get hpa --watch 10. load test ab -c 3 -n 2000000 http://xxxxxx.ap-southeast-1.elb.amazonaws.com/index.jsp 11. create cluster autoscaler create cluster autoscaler kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml add annotation kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict=\u0026quot;false\u0026quot; edit the Cluster Autoscaler deployment kubectl -n kube-system edit deployment.apps/cluster-autoscaler  replace with your cluster\u0026rsquo;s name add below  - --balance-similar-node-groups - --skip-nodes-with-system-pods=false like\nspec: containers: - command: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=aws - --skip-nodes-with-local-storage=false - --expander=least-waste - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/\u0026lt;YOUR CLUSTER NAME\u0026gt; - --balance-similar-node-groups - --skip-nodes-with-system-pods=false update image kubectl -n kube-system set image deployment.apps/cluster-autoscaler cluster-autoscaler=asia.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v1.16.5 cluster autoscaler logs kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler "
},
{
	"uri": "https://xuemark.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://xuemark.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]