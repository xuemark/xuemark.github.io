[
{
	"uri": "https://xuemark.github.io/",
	"title": "EKS Workshop",
	"tags": [],
	"description": "",
	"content": "EKS Workshop Mark Xue contributed\n "
},
{
	"uri": "https://xuemark.github.io/1.login-to-aws-console/",
	"title": "1. Login to AWS Console",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n  在浏览器中，打开链接https://dashboard.eventengine.run/login?hash=xxxxxx 点击“Readme”，打开EKS Workshop实验网站 点击“AWS Console” 点击“Open AWS Console” 点击左上角“Services”，然后搜索“Systems Manager” 点击打开Systems Manager 在左面导航栏中，找到Session Manager 点击进入Session Manager 点击右侧“Start Session” 选中“TestInstance”，点击“Start Session” 进入了EC2 instance。 执行下列命令  export PS1=\u0026quot;\\n[\\u@\\h \\W]$ \u0026quot; cd /home/ssm-user sudo su export AWS_REGION=ap-southeast-1 export CLUSTER_NAME=eks-test export NODEGROUP=$CLUSTER_NAME-nodegroup export TOMCAT_VERSION=9.0.37 "
},
{
	"uri": "https://xuemark.github.io/2.install-tools-for-eks/",
	"title": "2. Linux Install Tools For EKS",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n yum install yum install -y jq yum install -y httpd-tools install eksctl curl -OL \u0026quot;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026quot; tar -zxf eksctl_$(uname -s)_amd64.tar.gz mv ./eksctl /usr/bin eksctl version install kubectl curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.17.9/2020-08-04/bin/linux/amd64/kubectl chmod +x ./kubectl mv ./kubectl /usr/bin kubectl version "
},
{
	"uri": "https://xuemark.github.io/3.create-eks-cluster-and-nodegroup/",
	"title": "3. Create EKS Cluster and Nodegroup",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n 1. Environment export AWS_REGION=ap-southeast-1 export CLUSTER_NAME=eks-test export NODEGROUP=$CLUSTER_NAME-nodegroup export TOMCAT_VERSION=9.0.37 2. Create EKS Cluster and Nodegroup Create Cluster Script\ncat \u0026lt;\u0026lt;EOF \u0026gt; create_cluster.sh eksctl create cluster \\ --name $CLUSTER_NAME \\ --version 1.17 \\ --region $AWS_REGION \\ --vpc-public-subnets xxx \\ --vpc-private-subnets yyy \\ --nodegroup-name $CLUSTER_NAME-nodegroup \\ --node-type t3.small \\ --node-volume-size 5 \\ --nodes 1 \\ --nodes-min 1 \\ --nodes-max 5 \\ --node-private-networking \\ --alb-ingress-access \\ --managed \\ --asg-access \\ --full-ecr-access EOF Get and replace subnet id\nsubnets=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=\u0026quot;Public Subnet 1\u0026quot;,\u0026quot;Public Subnet 2\u0026quot;,\u0026quot;Public Subnet 3\u0026quot; --region $AWS_REGION --query \u0026quot;Subnets[*].{SubnetId:SubnetId}\u0026quot; --o text) subnets=${subnets//[[:space:]]/,} sed -i \u0026quot;s/xxx/${subnets}/\u0026quot; create_cluster.sh subnets=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=\u0026quot;Private Subnet 1\u0026quot;,\u0026quot;Private Subnet 2\u0026quot;,\u0026quot;Private Subnet 3\u0026quot; --region $AWS_REGION --query \u0026quot;Subnets[*].{SubnetId:SubnetId}\u0026quot; --o text) subnets=${subnets//[[:space:]]/,} sed -i \u0026quot;s/yyy/${subnets}/\u0026quot; create_cluster.sh cat create_cluster.sh Run create_cluster.sh\nsh create_cluster.sh wait about 10-15 minutes\n3. check node group status kubectl get node output\nNAME STATUS ROLES AGE VERSION ip-192-168-14-19.cn-northwest-1.compute.internal Ready \u0026lt;none\u0026gt; 4d1h v1.14.9-eks-1f0ca9 "
},
{
	"uri": "https://xuemark.github.io/4.docker-image-build/",
	"title": "4. Docker Imager Build",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n Build Docker Image including JAVA, Tomcat and AWSCLI\n1. install docker yum install -y docker systemctl start docker 2. download and install tomcat curl -o tomcat.tar.gz https://ftp.tsukuba.wide.ad.jp/software/apache/tomcat/tomcat-9/v${TOMCAT_VERSION}/bin/apache-tomcat-${TOMCAT_VERSION}.tar.gz tar -xzf tomcat.tar.gz mv apache-tomcat-${TOMCAT_VERSION} tomcat 3. make dockerfile cat \u0026lt;\u0026lt;EOF \u0026gt; dockerfile from centos:8 WORKDIR /home COPY tomcat/ /home/tomcat/ RUN yum install -y java-1.8.0-openjdk \u0026amp;\u0026amp;\\ yum install -y python36 \u0026amp;\u0026amp;\\ pip3 install awscli EXPOSE 8080 ENTRYPOINT [\u0026quot;/home/tomcat/bin/catalina.sh\u0026quot;,\u0026quot;run\u0026quot;] EOF 4. docker build docker build -t mytomcat . docker images output\nEPOSITORY TAG IMAGE ID CREATED SIZE mytomcat latest 65916bfdf6b7 7 seconds ago 550MB centos 8 470671670cac 4 months ago 237MB 5. docker run docker run -d -p 8080:8080 --name mytomcat-container mytomcat curl -I http://localhost:8080 6. upload docker image to ECR Initiate Elastic Container Registry  点击左上角“Services”，搜索“Elastic Container Registry” 进入Elastic Container Registry 点击右侧“get started” 在Repository name处，输入mytomcat，然后点击“Create repository”  upload docker image export ECR_URL=$(aws sts get-caller-identity| jq -r '.Account').dkr.ecr.ap-southeast-1.amazonaws.com/mytomcat:1 $(aws ecr get-login --no-include-email --region ap-southeast-1) docker tag mytomcat:latest $ECR_URL docker push $ECR_URL "
},
{
	"uri": "https://xuemark.github.io/5.create-app/",
	"title": "5. Create APP Deployment Service",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n 1. make app myapp.yml APP Yaml\ncat \u0026lt;\u0026lt;EOF \u0026gt; myapp.yml --- apiVersion: apps/v1 kind: Deployment metadata: name: myapp-deployment labels: app: myapp spec: selector: matchLabels: app: myapp replicas: 1 template: metadata: labels: app: myapp spec: containers: - name: mytomcat image: xxx ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: \u0026quot;myapp-service\u0026quot; spec: selector: app: myapp type: ClusterIP ports: - protocol: TCP port: 8080 targetPort: 8080 EOF update image path\nsed -i \u0026quot;s#xxx#${ECR_URL}#\u0026quot; myapp.yml 2. build deploy/service kubectl apply -f myapp.yml 3. check deployment and service kubectl get deploy kubectl get pod -o wide kubectl get service -o wide "
},
{
	"uri": "https://xuemark.github.io/6.create-alb-ingress-controller/",
	"title": "6. Create ALB Ingress Controller",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n 1. Create EKS OIDC Provider eksctl utils associate-iam-oidc-provider --cluster=${CLUSTER_NAME} --approve --region ${AWS_REGION} 2. Create IAM Policy create iam-policy-v1.1.5.json cat \u0026lt;\u0026lt;EOF \u0026gt; iam-policy-v1.1.5.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;acm:DescribeCertificate\u0026quot;, \u0026quot;acm:ListCertificates\u0026quot;, \u0026quot;acm:GetCertificate\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:AuthorizeSecurityGroupIngress\u0026quot;, \u0026quot;ec2:CreateSecurityGroup\u0026quot;, \u0026quot;ec2:CreateTags\u0026quot;, \u0026quot;ec2:DeleteTags\u0026quot;, \u0026quot;ec2:DeleteSecurityGroup\u0026quot;, \u0026quot;ec2:DescribeAccountAttributes\u0026quot;, \u0026quot;ec2:DescribeAddresses\u0026quot;, \u0026quot;ec2:DescribeInstances\u0026quot;, \u0026quot;ec2:DescribeInstanceStatus\u0026quot;, \u0026quot;ec2:DescribeInternetGateways\u0026quot;, \u0026quot;ec2:DescribeNetworkInterfaces\u0026quot;, \u0026quot;ec2:DescribeSecurityGroups\u0026quot;, \u0026quot;ec2:DescribeSubnets\u0026quot;, \u0026quot;ec2:DescribeTags\u0026quot;, \u0026quot;ec2:DescribeVpcs\u0026quot;, \u0026quot;ec2:ModifyInstanceAttribute\u0026quot;, \u0026quot;ec2:ModifyNetworkInterfaceAttribute\u0026quot;, \u0026quot;ec2:RevokeSecurityGroupIngress\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;elasticloadbalancing:AddListenerCertificates\u0026quot;, \u0026quot;elasticloadbalancing:AddTags\u0026quot;, \u0026quot;elasticloadbalancing:CreateListener\u0026quot;, \u0026quot;elasticloadbalancing:CreateLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:CreateRule\u0026quot;, \u0026quot;elasticloadbalancing:CreateTargetGroup\u0026quot;, \u0026quot;elasticloadbalancing:DeleteListener\u0026quot;, \u0026quot;elasticloadbalancing:DeleteLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:DeleteRule\u0026quot;, \u0026quot;elasticloadbalancing:DeleteTargetGroup\u0026quot;, \u0026quot;elasticloadbalancing:DeregisterTargets\u0026quot;, \u0026quot;elasticloadbalancing:DescribeListenerCertificates\u0026quot;, \u0026quot;elasticloadbalancing:DescribeListeners\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancers\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancerAttributes\u0026quot;, \u0026quot;elasticloadbalancing:DescribeRules\u0026quot;, \u0026quot;elasticloadbalancing:DescribeSSLPolicies\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTags\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTargetGroups\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTargetGroupAttributes\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTargetHealth\u0026quot;, \u0026quot;elasticloadbalancing:ModifyListener\u0026quot;, \u0026quot;elasticloadbalancing:ModifyLoadBalancerAttributes\u0026quot;, \u0026quot;elasticloadbalancing:ModifyRule\u0026quot;, \u0026quot;elasticloadbalancing:ModifyTargetGroup\u0026quot;, \u0026quot;elasticloadbalancing:ModifyTargetGroupAttributes\u0026quot;, \u0026quot;elasticloadbalancing:RegisterTargets\u0026quot;, \u0026quot;elasticloadbalancing:RemoveListenerCertificates\u0026quot;, \u0026quot;elasticloadbalancing:RemoveTags\u0026quot;, \u0026quot;elasticloadbalancing:SetIpAddressType\u0026quot;, \u0026quot;elasticloadbalancing:SetSecurityGroups\u0026quot;, \u0026quot;elasticloadbalancing:SetSubnets\u0026quot;, \u0026quot;elasticloadbalancing:SetWebACL\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;iam:CreateServiceLinkedRole\u0026quot;, \u0026quot;iam:GetServerCertificate\u0026quot;, \u0026quot;iam:ListServerCertificates\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;cognito-idp:DescribeUserPoolClient\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;waf-regional:GetWebACLForResource\u0026quot;, \u0026quot;waf-regional:GetWebACL\u0026quot;, \u0026quot;waf-regional:AssociateWebACL\u0026quot;, \u0026quot;waf-regional:DisassociateWebACL\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;tag:GetResources\u0026quot;, \u0026quot;tag:TagResources\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;waf:GetWebACL\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } EOF create IAM policy aws iam create-policy --policy-name ALBIngressControllerIAMPolicy \\ --policy-document file://./iam-policy-v1.1.5.json --region ${AWS_REGION} export ENV POLICY_NAME=$(aws iam list-policies --query 'Policies[?PolicyName==`ALBIngressControllerIAMPolicy`].Arn' --output text --region ${AWS_REGION}) 3.create service account eksctl create iamserviceaccount \\ --cluster=${CLUSTER_NAME} \\ --namespace=kube-system \\ --name=alb-ingress-controller \\ --attach-policy-arn=${POLICY_NAME} \\ --override-existing-serviceaccounts \\ --approve 4.create RBAC create rbac-role-v1.1.5.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; rbac-role-v1.1.5.yaml --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/name: alb-ingress-controller name: alb-ingress-controller rules: - apiGroups: - \u0026quot;\u0026quot; - extensions resources: - configmaps - endpoints - events - ingresses - ingresses/status - services verbs: - create - get - list - update - watch - patch - apiGroups: - \u0026quot;\u0026quot; - extensions resources: - nodes - pods - secrets - services - namespaces verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/name: alb-ingress-controller name: alb-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: alb-ingress-controller subjects: - kind: ServiceAccount name: alb-ingress-controller namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: labels: app.kubernetes.io/name: alb-ingress-controller name: alb-ingress-controller namespace: kube-system ... EOF create RBAC kubectl apply -f rbac-role-v1.1.5.yaml 5. create ALB APP create alb-ingress-controller-v1.1.5.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; alb-ingress-controller-v1.1.5.yaml # Application Load Balancer (ALB) Ingress Controller Deployment Manifest. # This manifest details sensible defaults for deploying an ALB Ingress Controller. # GitHub: https://github.com/kubernetes-sigs/aws-alb-ingress-controller apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: alb-ingress-controller name: alb-ingress-controller # Namespace the ALB Ingress Controller should run in. Does not impact which # namespaces it's able to resolve ingress resource for. For limiting ingress # namespace scope, see --watch-namespace. namespace: kube-system spec: selector: matchLabels: app.kubernetes.io/name: alb-ingress-controller template: metadata: labels: app.kubernetes.io/name: alb-ingress-controller spec: containers: - name: alb-ingress-controller args: # Limit the namespace where this ALB Ingress Controller deployment will # resolve ingress resources. If left commented, all namespaces are used. # - --watch-namespace=your-k8s-namespace # Setting the ingress-class flag below ensures that only ingress resources with the # annotation kubernetes.io/ingress.class: \u0026quot;alb\u0026quot; are respected by the controller. You may # choose any class you'd like for this controller to respect. - --ingress-class=alb # REQUIRED # Name of your cluster. Used when naming resources created # by the ALB Ingress Controller, providing distinction between # clusters. - --cluster-name=eks-test # AWS VPC ID this ingress controller will use to create AWS resources. # If unspecified, it will be discovered from ec2metadata. - --aws-vpc-id=xxx # AWS region this ingress controller will operate in. # If unspecified, it will be discovered from ec2metadata. # List of regions: http://docs.aws.amazon.com/general/latest/gr/rande.html#vpc_region - --aws-region=ap-southeast-1 # Enables logging on all outbound requests sent to the AWS API. # If logging is desired, set to true. # - --aws-api-debug # Maximum number of times to retry the aws calls. # defaults to 10. # - --aws-max-retries=10 # env: # AWS key id for authenticating with the AWS API. # This is only here for examples. It's recommended you instead use # a project like kube2iam for granting access. #- name: AWS_ACCESS_KEY_ID # value: KEYVALUE # AWS key secret for authenticating with the AWS API. # This is only here for examples. It's recommended you instead use # a project like kube2iam for granting access. #- name: AWS_SECRET_ACCESS_KEY # value: SECRETVALUE # Repository location of the ALB Ingress Controller. image: docker.io/amazon/aws-alb-ingress-controller:v1.1.5 serviceAccountName: alb-ingress-controller EOF update vpc id\nexport VPCID=$(eksctl get cluster --name ${CLUSTER_NAME} --region ${AWS_REGION} -o json | jq -r '.[].ResourcesVpcConfig.VpcId') sed -i \u0026quot;s#xxx#${VPCID}#\u0026quot; alb-ingress-controller-v1.1.5.yaml create ALB deployment kubectl apply -f alb-ingress-controller-v1.1.5.yaml view pod kubectl get pod -A 6. create ingress for service - create ALB add subnet tags subnets=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=\u0026quot;Public Subnet 1\u0026quot;,\u0026quot;Public Subnet 2\u0026quot;,\u0026quot;Public Subnet 3\u0026quot; --region $AWS_REGION --query \u0026quot;Subnets[*].{SubnetId:SubnetId}\u0026quot; --o text) aws ec2 create-tags --resources ${subnets} --region ${AWS_REGION} --tags Key=kubernetes.io/role/elb,Value=1 create alb-ingress.yml cat \u0026lt;\u0026lt;EOF \u0026gt; alb-ingress.yml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: \u0026quot;alb-ingress\u0026quot; namespace: \u0026quot;default\u0026quot; annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip labels: app: myapp spec: rules: - http: paths: - path: /* backend: serviceName: \u0026quot;myapp-service\u0026quot; servicePort: 8080 EOF create ALB kubectl apply -f alb-ingress.yml 7. get ALB URL kubectl get ingress output\nAME HOSTS ADDRESS PORTS AGE alb-ingress * xxx.ap-southeast-1.elb.amazonaws.com 80 13m in browser, input below:\nhttp://xxx.ap-southeast-1.elb.amazonaws.com "
},
{
	"uri": "https://xuemark.github.io/7.monitor/",
	"title": "7. Monitoring EKS",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n 1. add policy to role export STACK_NAME=$(eksctl get nodegroup --cluster ${CLUSTER_NAME} --region ${AWS_REGION} -o json | jq -r '.[].StackName') export ROLE_NAME=$(aws cloudformation describe-stack-resources --stack-name $STACK_NAME --region ${AWS_REGION}| jq -r '.StackResources[] | select(.ResourceType==\u0026quot;AWS::IAM::Role\u0026quot;) | .PhysicalResourceId') echo ${ROLE_NAME} aws iam attach-role-policy \\ --role-name $ROLE_NAME \\ --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy 2. install cloudwatch agent curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluentd-quickstart.yaml | sed \u0026quot;s/{{cluster_name}}/${CLUSTER_NAME}/;s/{{region_name}}/${AWS_REGION}/\u0026quot; | kubectl apply -f - 3. view cloudwatch agent kubectl get pod -n amazon-cloudwatch 4. view cloudwatch container insight  点击左上角“Services”，然后搜索“CloudWatch” 点击打开CloudWatch 在左面导航栏中，找到Container Insights 点击进入Performance Monitoring 左边下拉框选择“EKS Service”  "
},
{
	"uri": "https://xuemark.github.io/8.create-service-account/",
	"title": "8. Create service account for pod",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n 1. create service account eksctl create iamserviceaccount --name s3-full-access --namespace default \\ --cluster ${CLUSTER_NAME} --attach-policy-arn arn:aws-cn:iam::aws:policy/AmazonS3FullAccess \\ --approve --override-existing-serviceaccounts --region ${AWS_REGION} 2. create S3 bucket modify S3_BUCKET with unique name\nS3_BUCKET=ekstest20200805 create bucket\nif [ $(aws s3 ls | grep $S3_BUCKET | wc -l) -eq 0 ]; then aws s3 mb s3://$S3_BUCKET --region $AWS_REGION echo \u0026quot;test\u0026quot; \u0026gt; test.txt aws s3 cp test.txt s3://$S3_BUCKET/ else echo \u0026quot;S3 bucket $S3_BUCKET existed, skip creation\u0026quot; fi 3. pod test without s3 permission make pod yaml without s3 permission\ncat \u0026lt;\u0026lt;EOF \u0026gt; mypod.yml apiVersion: v1 kind: Pod metadata: name: mytomcatpod-1 labels: name: mytomcatpod spec: hostname: mytomcatpod-1 subdomain: mytomcatpod containers: - name: mytomcatpod image: xxx ports: - containerPort: 8080 EOF update image url\nsed -i \u0026quot;s#xxx#${ECR_URL}#\u0026quot; mypod.yml create pod\nkubectl apply -f mypod.yml enter pod\nkubectl exec -it mytomcatpod-1 /bin/bash check s3 permission, replace \u0026lt;S3_BUCKET\u0026gt;\naws s3 ls s3://\u0026lt;S3_BUCKET\u0026gt; output\nAn error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied 4. pod test with s3 permission make pod yaml with s3 permission\ncat \u0026lt;\u0026lt;EOF \u0026gt; mypods3.yml apiVersion: v1 kind: Pod metadata: name: mytomcatpod-s3-1 labels: name: mytomcatpod-s3 spec: hostname: mytomcatpod-s3-1 subdomain: mytomcatpod-s3 containers: - name: mytomcatpod-s3 image: xxx ports: - containerPort: 8080 serviceAccountName: s3-full-access EOF update image url\nsed -i \u0026quot;s#xxx#${ECR_URL}#\u0026quot; mypods3.yml create pod\nkubectl apply -f mypods3.yml enter pod\nkubectl exec -it mytomcatpod-s3-1 /bin/bash check s3 permission, replace \u0026lt;S3_BUCKET\u0026gt;\naws s3 ls s3://\u0026lt;S3_BUCKET\u0026gt; output\n2020-08-04 15:54:02 5 test.txt 5. cleanup kubectl delete -f mypod.yml kubectl delete -f mypods3.yml "
},
{
	"uri": "https://xuemark.github.io/9.scale-pod-node/",
	"title": "9. Scale pod and node",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n Configure HPA 1. manually scale pod kubectl scale --replicas=10 deployment/myapp-deployment kubectl get pod -o wide Check EC2\u0026gt;LOAD BALANCING\u0026gt;Target Group\u0026gt;Targets\n2. check pod status There are 6 pod in Pending status.\nkubectl get pod kubectl get pod -o json | jq -r '.items[] | select(.status.phase==\u0026quot;Pending\u0026quot;)' | jq -r '.metadata.name' kubectl get events 3. scale node group scale node instance to 2\neksctl scale nodegroup --cluster=${CLUSTER_NAME} --nodes=2 --name=${NODEGROUP} --region=${AWS_REGION} kubectl get node 4. check pod status All pods are in Running status.\nkubectl get pod -o wide 5. create HPA curl -sL https://api.github.com/repos/kubernetes-sigs/metrics-server/tarball/v0.3.6 -o metrics-server-v0.3.6.tar.gz tar -xzf metrics-server-v0.3.6.tar.gz kubectl apply -f kubernetes-sigs-metrics-server-d1f4f6f/deploy/1.8+/ kubectl get pod --all-namespaces 6. scale down pod kubectl scale --replicas=1 deployment/myapp-deployment 7. set limit for pod kubectl set resources deployment myapp-deployment --limits=cpu=200m,memory=256Mi 8. set autoscale policy kubectl autoscale deployment myapp-deployment --cpu-percent=30 --min=1 --max=20 9. monitor HPA kubectl top pod kubectl get hpa --watch 10. load test open another session, replace url\nALB_URL=$(kubectl get ingress -o json| jq -r '.items[0].status.loadBalancer.ingress[0].hostname') ab -c 10 -n 2000000 http://${ALB_URL}/index.jsp 11. stop load  shut down ab test scale in pod  kubectl delete hpa myapp-deployment kubectl scale --replicas=1 deployment/myapp-deployment view pod count , should be 1\nkubectl get pod re-create hpa\nkubectl autoscale deployment myapp-deployment --cpu-percent=30 --min=1 --max=20 Configure CA 1. create cluster autoscaler make ca yaml cat \u0026lt;\u0026lt;EOF \u0026gt; cluster-autoscaler-autodiscover.yaml --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-addon: cluster-autoscaler.addons.k8s.io k8s-app: cluster-autoscaler name: cluster-autoscaler namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-autoscaler labels: k8s-addon: cluster-autoscaler.addons.k8s.io k8s-app: cluster-autoscaler rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;events\u0026quot;, \u0026quot;endpoints\u0026quot;] verbs: [\u0026quot;create\u0026quot;, \u0026quot;patch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;pods/eviction\u0026quot;] verbs: [\u0026quot;create\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;pods/status\u0026quot;] verbs: [\u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;endpoints\u0026quot;] resourceNames: [\u0026quot;cluster-autoscaler\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;nodes\u0026quot;] verbs: [\u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;get\u0026quot;, \u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: - \u0026quot;pods\u0026quot; - \u0026quot;services\u0026quot; - \u0026quot;replicationcontrollers\u0026quot; - \u0026quot;persistentvolumeclaims\u0026quot; - \u0026quot;persistentvolumes\u0026quot; verbs: [\u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;get\u0026quot;] - apiGroups: [\u0026quot;extensions\u0026quot;] resources: [\u0026quot;replicasets\u0026quot;, \u0026quot;daemonsets\u0026quot;] verbs: [\u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;get\u0026quot;] - apiGroups: [\u0026quot;policy\u0026quot;] resources: [\u0026quot;poddisruptionbudgets\u0026quot;] verbs: [\u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;] - apiGroups: [\u0026quot;apps\u0026quot;] resources: [\u0026quot;statefulsets\u0026quot;, \u0026quot;replicasets\u0026quot;, \u0026quot;daemonsets\u0026quot;] verbs: [\u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;get\u0026quot;] - apiGroups: [\u0026quot;storage.k8s.io\u0026quot;] resources: [\u0026quot;storageclasses\u0026quot;, \u0026quot;csinodes\u0026quot;] verbs: [\u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;get\u0026quot;] - apiGroups: [\u0026quot;batch\u0026quot;, \u0026quot;extensions\u0026quot;] resources: [\u0026quot;jobs\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;patch\u0026quot;] - apiGroups: [\u0026quot;coordination.k8s.io\u0026quot;] resources: [\u0026quot;leases\u0026quot;] verbs: [\u0026quot;create\u0026quot;] - apiGroups: [\u0026quot;coordination.k8s.io\u0026quot;] resourceNames: [\u0026quot;cluster-autoscaler\u0026quot;] resources: [\u0026quot;leases\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;update\u0026quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: cluster-autoscaler namespace: kube-system labels: k8s-addon: cluster-autoscaler.addons.k8s.io k8s-app: cluster-autoscaler rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;configmaps\u0026quot;] verbs: [\u0026quot;create\u0026quot;,\u0026quot;list\u0026quot;,\u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;configmaps\u0026quot;] resourceNames: [\u0026quot;cluster-autoscaler-status\u0026quot;, \u0026quot;cluster-autoscaler-priority-expander\u0026quot;] verbs: [\u0026quot;delete\u0026quot;, \u0026quot;get\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;watch\u0026quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-autoscaler labels: k8s-addon: cluster-autoscaler.addons.k8s.io k8s-app: cluster-autoscaler roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-autoscaler subjects: - kind: ServiceAccount name: cluster-autoscaler namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cluster-autoscaler namespace: kube-system labels: k8s-addon: cluster-autoscaler.addons.k8s.io k8s-app: cluster-autoscaler roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cluster-autoscaler subjects: - kind: ServiceAccount name: cluster-autoscaler namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: cluster-autoscaler namespace: kube-system labels: app: cluster-autoscaler spec: replicas: 1 selector: matchLabels: app: cluster-autoscaler template: metadata: labels: app: cluster-autoscaler annotations: prometheus.io/scrape: 'true' prometheus.io/port: '8085' spec: serviceAccountName: cluster-autoscaler containers: - image: k8s.gcr.io/cluster-autoscaler:v1.14.7 name: cluster-autoscaler resources: limits: cpu: 100m memory: 300Mi requests: cpu: 100m memory: 300Mi command: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=aws - --skip-nodes-with-local-storage=false - --expander=least-waste - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/xxx - --balance-similar-node-groups - --skip-nodes-with-system-pods=false volumeMounts: - name: ssl-certs mountPath: /etc/ssl/certs/ca-certificates.crt readOnly: true imagePullPolicy: \u0026quot;Always\u0026quot; volumes: - name: ssl-certs hostPath: path: \u0026quot;/etc/ssl/certs/ca-bundle.crt\u0026quot; EOF update cluster name sed -i \u0026quot;s#xxx#${CLUSTER_NAME}#\u0026quot; cluster-autoscaler-autodiscover.yaml create cluster autoscaler kubectl apply -f cluster-autoscaler-autodiscover.yaml add annotation kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict=\u0026quot;false\u0026quot; update image kubectl -n kube-system set image deployment.apps/cluster-autoscaler cluster-autoscaler=asia.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v1.16.5 cluster autoscaler logs kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler 2. load test ab -c 10 -n 2000000 http://${ALB_URL}/index.jsp 3. monitor HPA kubectl top pod kubectl get hpa --watch 4. check nod count kubectl get node "
},
{
	"uri": "https://xuemark.github.io/10.advanced/",
	"title": "10. Advanced",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n "
},
{
	"uri": "https://xuemark.github.io/10.advanced/1.premetheus-grafana/",
	"title": "1. Prometheus and Grafana Monitoring",
	"tags": [],
	"description": "",
	"content": " This is link for return home page.\n 1. install helm  linux  curl -sSL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash  macos  brew install helm  helm add repository  helm repo list helm repo remove stable helm repo update helm repo add stable https://burdenbear.github.io/kube-charts-mirror/ 2. install Prometheus  Create a Prometheus namespace.  kubectl create namespace prometheus  Deploy Prometheus with Helm.  helm install prometheus stable/prometheus \\ --namespace prometheus \\ --set alertmanager.persistentVolume.storageClass=\u0026quot;gp2\u0026quot;,server.persistentVolume.storageClass=\u0026quot;gp2\u0026quot; output: The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster: prometheus-server.prometheus.svc.cluster.local\n Use kubectl to port forward the Prometheus console to your local machine.  kubectl --namespace=prometheus port-forward deploy/prometheus-server 9090  see Prometheus metrics  http://localhost:9090 choose Status\u0026gt;Targets 3. install Grafana  create namespace  kubectl create namespace grafana  Deploy Grafana  helm install grafana stable/grafana \\ --namespace grafana \\ --set persistence.storageClassName=\u0026quot;gp2\u0026quot; \\ --set adminPassword='EKS!sAWSome' \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.apiVersion=1 \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].name=Prometheus \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].type=prometheus \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].url=http://prometheus-server.prometheus.svc.cluster.local \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].access=proxy \\ --set datasources.\u0026quot;datasources\\.yaml\u0026quot;.datasources[0].isDefault=true \\ --set service.type=ClusterIP  Use kubectl to port forward the Grafana console to your local machine.  kubectl --namespace=grafana port-forward deploy/grafana 3000  see Grafana  http://localhost:3000  Email or username = admin Password = EKS!sAWSome  4. grafana import dashboard  查看所有集群节点的监控面板  左侧面板点击' + '，选择' Import ' Grafana.com Dashboard下输入3119 prometheus data source下拉框中选择prometheus 点击' Import '  查看Pods的监控面板  左侧面板点击' + '，选择' Import ' Grafana.com Dashboard下输6417 输入Kubernetes Pods Monitoring作为Dashboard名称 点击change，设置uid prometheus data source下拉框中选择prometheus 点击' Import ' 5. cleanup helm uninstall grafana --namespace grafana helm uninstall prometheus --namespace prometheus kubectl delete ns grafana kubectl delete ns prometheus "
},
{
	"uri": "https://xuemark.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://xuemark.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]